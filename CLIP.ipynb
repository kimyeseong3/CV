{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CLIP (ì „ì²´ ëª¨ë¸) ì„¤ëª…"
      ],
      "metadata": {
        "id": "PD6dJlH8E10B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP2A9s7sEo9L"
      },
      "outputs": [],
      "source": [
        "class CLIP(nn.Module):\n",
        "    \"\"\"\n",
        "    CLIP ì „ì²´ ëª¨ë¸\n",
        "\n",
        "    [êµ¬ì„±]\n",
        "    1. Image Encoder (ResNet or ViT)\n",
        "    2. Text Encoder (Transformer)\n",
        "    3. Learnable temperature parameter\n",
        "\n",
        "    [ì¶œì²˜] CLIP ë…¼ë¬¸: https://arxiv.org/abs/2103.00020\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 embed_dim: int,               # ê³µí†µ ì„ë² ë”© ì°¨ì› (ì˜ˆ: 512)\n",
        "                 # ===== Vision ê´€ë ¨ =====\n",
        "                 image_resolution: int,         # ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° (ì˜ˆ: 224)\n",
        "                 vision_layers: Union[Tuple[int, int, int, int], int],  # ResNet: tuple, ViT: int\n",
        "                 vision_width: int,             # Vision encoderì˜ base width\n",
        "                 vision_patch_size: int,        # ViT íŒ¨ì¹˜ í¬ê¸° (ResNetì´ë©´ None)\n",
        "                 # ===== Text ê´€ë ¨ =====\n",
        "                 context_length: int,           # ìµœëŒ€ í† í° ê¸¸ì´ (ì˜ˆ: 77)\n",
        "                 vocab_size: int,               # ì–´íœ˜ í¬ê¸° (ì˜ˆ: 49408)\n",
        "                 transformer_width: int,        # Transformer hidden dimension\n",
        "                 transformer_heads: int,        # Attention head ê°œìˆ˜\n",
        "                 transformer_layers: int        # Transformer layer ê°œìˆ˜\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.context_length = context_length\n",
        "\n",
        "        # ========================================\n",
        "        # Image Encoder ì„ íƒ: ResNet or ViT\n",
        "        # ========================================\n",
        "        if isinstance(vision_layers, (tuple, list)):\n",
        "            # ===== Option 1: Modified ResNet =====\n",
        "            # vision_layersê°€ tupleì´ë©´ ResNet\n",
        "            # ì˜ˆ: (3, 4, 6, 3) -> ResNet-50\n",
        "            vision_heads = vision_width * 32 // 64  # Attention head ê°œìˆ˜ ê³„ì‚°\n",
        "            self.visual = ModifiedResNet(\n",
        "                layers=vision_layers,          # ê° stageì˜ ë¸”ë¡ ê°œìˆ˜\n",
        "                output_dim=embed_dim,          # ìµœì¢… ì¶œë ¥ ì°¨ì›\n",
        "                heads=vision_heads,            # AttentionPoolì˜ head ê°œìˆ˜\n",
        "                input_resolution=image_resolution,\n",
        "                width=vision_width             # Base channel width\n",
        "            )\n",
        "        else:\n",
        "            # ===== Option 2: Vision Transformer =====\n",
        "            # vision_layersê°€ intë©´ ViT\n",
        "            # ì˜ˆ: 12 -> 12-layer ViT\n",
        "            vision_heads = vision_width // 64\n",
        "            self.visual = VisionTransformer(\n",
        "                input_resolution=image_resolution,  # 224\n",
        "                patch_size=vision_patch_size,       # 16 or 32\n",
        "                width=vision_width,                 # Hidden dimension\n",
        "                layers=vision_layers,               # Transformer layers\n",
        "                heads=vision_heads,\n",
        "                output_dim=embed_dim\n",
        "            )\n",
        "\n",
        "        # ========================================\n",
        "        # Text Encoder\n",
        "        # ========================================\n",
        "        # Causal attention mask ìƒì„±\n",
        "        # ê° í† í°ì´ ìê¸° ìì‹ ê³¼ ì´ì „ í† í°ë§Œ ë³¼ ìˆ˜ ìˆê²Œ\n",
        "        self.transformer = Transformer(\n",
        "            width=transformer_width,           # ì˜ˆ: 512\n",
        "            layers=transformer_layers,         # ì˜ˆ: 12\n",
        "            heads=transformer_heads,           # ì˜ˆ: 8\n",
        "            attn_mask=self.build_attention_mask()  # Causal mask\n",
        "        )\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # ===== Token Embedding =====\n",
        "        # ë‹¨ì–´ ID -> ì„ë² ë”© ë²¡í„°\n",
        "        # ì˜ˆ: vocab_size=49408, transformer_width=512\n",
        "        # -> 49408ê°œ ë‹¨ì–´ ê°ê°ì„ 512ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜\n",
        "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
        "\n",
        "        # ===== Positional Embedding =====\n",
        "        # ê° ìœ„ì¹˜(0~76)ì— ëŒ€í•œ í•™ìŠµ ê°€ëŠ¥í•œ ì„ë² ë”©\n",
        "        # TransformerëŠ” ìˆœì„œ ì •ë³´ê°€ ì—†ì–´ì„œ ìœ„ì¹˜ ì •ë³´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€\n",
        "        self.positional_embedding = nn.Parameter(\n",
        "            torch.empty(self.context_length, transformer_width)\n",
        "        )\n",
        "\n",
        "        # ===== Final Layer Norm =====\n",
        "        self.ln_final = LayerNorm(transformer_width)\n",
        "\n",
        "        # ===== Text Projection =====\n",
        "        # Transformer ì¶œë ¥(transformer_width)ì„ ì´ë¯¸ì§€ì™€ ê°™ì€ ì°¨ì›(embed_dim)ìœ¼ë¡œ\n",
        "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
        "\n",
        "        # ========================================\n",
        "        # Temperature Parameter (ë…¼ë¬¸ì˜ Ï„)\n",
        "        # ========================================\n",
        "        # Contrastive learningì˜ í•µì‹¬ hyperparameter\n",
        "        # í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¦! (ê³ ì •ê°’ì´ ì•„ë‹˜)\n",
        "        # ì´ˆê¸°ê°’: log(1/0.07) â‰ˆ 2.66\n",
        "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
        "\n",
        "        # ===== íŒŒë¼ë¯¸í„° ì´ˆê¸°í™” =====\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        \"\"\"\n",
        "        ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ì ì ˆí•œ ë¶„í¬ë¡œ ì´ˆê¸°í™”\n",
        "\n",
        "        [ì¤‘ìš”ì„±]\n",
        "        ì´ˆê¸°í™”ê°€ ì˜ëª»ë˜ë©´ í•™ìŠµì´ ë¶ˆì•ˆì •í•˜ê±°ë‚˜ ì•ˆ ë  ìˆ˜ ìˆìŒ\n",
        "        íŠ¹íˆ deep networkì—ì„œëŠ” ì´ˆê¸°í™”ê°€ critical\n",
        "        \"\"\"\n",
        "        # ===== Embedding ì´ˆê¸°í™” =====\n",
        "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
        "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
        "\n",
        "        # ===== Vision Encoder ì´ˆê¸°í™” =====\n",
        "        if isinstance(self.visual, ModifiedResNet):\n",
        "            # AttentionPool ì´ˆê¸°í™”\n",
        "            if self.visual.attnpool is not None:\n",
        "                std = self.visual.attnpool.c_proj.in_features ** -0.5\n",
        "                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)\n",
        "                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)\n",
        "                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)\n",
        "                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)\n",
        "\n",
        "            # ResNet block ì´ˆê¸°í™”\n",
        "            # bn3.weightë¥¼ 0ìœ¼ë¡œ -> residual pathë¥¼ ì²˜ìŒì—ëŠ” identityë¡œ\n",
        "            for resnet_block in [self.visual.layer1, self.visual.layer2,\n",
        "                                self.visual.layer3, self.visual.layer4]:\n",
        "                for name, param in resnet_block.named_parameters():\n",
        "                    if name.endswith(\"bn3.weight\"):\n",
        "                        nn.init.zeros_(param)\n",
        "\n",
        "        # ===== Text Encoder ì´ˆê¸°í™” =====\n",
        "        # Layerê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ ì‘ì€ stdë¡œ ì´ˆê¸°í™” (ì•ˆì •ì ì¸ í•™ìŠµ)\n",
        "        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n",
        "        attn_std = self.transformer.width ** -0.5\n",
        "        fc_std = (2 * self.transformer.width) ** -0.5\n",
        "\n",
        "        for block in self.transformer.resblocks:\n",
        "            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n",
        "            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n",
        "            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n",
        "            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
        "\n",
        "        # ===== Text Projection ì´ˆê¸°í™” =====\n",
        "        if self.text_projection is not None:\n",
        "            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n",
        "\n",
        "    def build_attention_mask(self):\n",
        "        \"\"\"\n",
        "        Causal attention mask ìƒì„±\n",
        "\n",
        "        [ëª©ì ]\n",
        "        ê° í† í°ì´ ìê¸° ìì‹ ê³¼ ì´ì „ í† í°ë§Œ ë³¼ ìˆ˜ ìˆê²Œ ì œí•œ\n",
        "        GPT ìŠ¤íƒ€ì¼ì˜ autoregressive attention\n",
        "\n",
        "        [ì¶œë ¥]\n",
        "        ìƒì‚¼ê° í–‰ë ¬ (upper triangular)\n",
        "        ëŒ€ê°ì„  í¬í•¨ ì•„ë˜ìª½ì€ 0, ìœ„ìª½ì€ -inf\n",
        "\n",
        "        ì˜ˆì‹œ (context_length=4):\n",
        "        [[  0, -inf, -inf, -inf],   <- í† í° 0ì€ ìê¸°ë§Œ ë³¼ ìˆ˜ ìˆìŒ\n",
        "         [  0,   0,  -inf, -inf],   <- í† í° 1ì€ 0,1ë§Œ ë³¼ ìˆ˜ ìˆìŒ\n",
        "         [  0,   0,    0,  -inf],   <- í† í° 2ëŠ” 0,1,2ë§Œ ë³¼ ìˆ˜ ìˆìŒ\n",
        "         [  0,   0,    0,    0 ]]   <- í† í° 3ì€ ëª¨ë‘ ë³¼ ìˆ˜ ìˆìŒ\n",
        "        \"\"\"\n",
        "        mask = torch.empty(self.context_length, self.context_length)\n",
        "        mask.fill_(float(\"-inf\"))  # ëª¨ë‘ -infë¡œ ì±„ì›€\n",
        "        mask.triu_(1)              # ëŒ€ê°ì„  ìœ„ìª½ë§Œ -infë¡œ (triu = triangular upper)\n",
        "        return mask\n",
        "\n",
        "    @property\n",
        "    def dtype(self):\n",
        "        \"\"\"ëª¨ë¸ì˜ dtype (fp16 or fp32)\"\"\"\n",
        "        return self.visual.conv1.weight.dtype\n",
        "\n",
        "    # ========================================\n",
        "    # ğŸ“· Image Encoding\n",
        "    # ========================================\n",
        "    def encode_image(self, image):\n",
        "        \"\"\"\n",
        "        ì´ë¯¸ì§€ë¥¼ ë²¡í„°ë¡œ ì¸ì½”ë”©\n",
        "\n",
        "        [ì…ë ¥] image: (batch, 3, 224, 224)\n",
        "        [ì¶œë ¥] (batch, embed_dim) - ì˜ˆ: (batch, 512)\n",
        "\n",
        "        [ê³¼ì •]\n",
        "        ì´ë¯¸ì§€ -> Visual Encoder (ResNet/ViT) -> embed_dim ë²¡í„°\n",
        "        \"\"\"\n",
        "        return self.visual(image.type(self.dtype))\n",
        "\n",
        "    # ========================================\n",
        "    # ğŸ“ Text Encoding\n",
        "    # ========================================\n",
        "    def encode_text(self, text):\n",
        "        \"\"\"\n",
        "        í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ì¸ì½”ë”©\n",
        "\n",
        "        [ì…ë ¥] text: (batch, context_length) - í† í° ID ì‹œí€€ìŠ¤\n",
        "              ì˜ˆ: [[49406, 320, 1929, 49407, 0, 0, ...]]\n",
        "                   [SoT]  [a]  [dog] [EoT] [PAD] ...\n",
        "        [ì¶œë ¥] (batch, embed_dim) - ì˜ˆ: (batch, 512)\n",
        "        \"\"\"\n",
        "\n",
        "        # ===== Step 1: Token Embedding =====\n",
        "        # í† í° ID -> ì„ë² ë”© ë²¡í„°\n",
        "        # (batch, 77) -> (batch, 77, 512)\n",
        "        x = self.token_embedding(text).type(self.dtype)\n",
        "\n",
        "        # ===== Step 2: Positional Embedding ì¶”ê°€ =====\n",
        "        # ê° ìœ„ì¹˜ì— ëŒ€í•œ ì •ë³´ ë”í•˜ê¸°\n",
        "        # (batch, 77, 512) + (77, 512) -> (batch, 77, 512)\n",
        "        x = x + self.positional_embedding.type(self.dtype)\n",
        "\n",
        "        # ===== Step 3: Transformer ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜ =====\n",
        "        # (batch, 77, 512) -> (77, batch, 512)\n",
        "        # TransformerëŠ” (sequence, batch, feature) í˜•íƒœë¥¼ ê¸°ëŒ€\n",
        "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
        "\n",
        "        # ===== Step 4: Transformer í†µê³¼ =====\n",
        "        # 12ê°œ layerë¥¼ ê±°ì¹˜ë©° ë¬¸ì¥ì˜ ì˜ë¯¸ íŒŒì•…\n",
        "        # (77, batch, 512) -> (77, batch, 512)\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # ===== Step 5: ë‹¤ì‹œ ì›ë˜ í˜•íƒœë¡œ =====\n",
        "        # (77, batch, 512) -> (batch, 77, 512)\n",
        "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
        "\n",
        "        # ===== Step 6: Final Layer Norm =====\n",
        "        x = self.ln_final(x).type(self.dtype)\n",
        "\n",
        "        # ===== Step 7: [EoT] í† í° ì¶”ì¶œ â­ í•µì‹¬! =====\n",
        "        # x.shape = [batch_size, 77, 512]\n",
        "        #\n",
        "        # [ì™œ EoT í† í°ì„ ì“°ëŠ”ê°€?]\n",
        "        # Causal attention ë•Œë¬¸ì— EoTëŠ” ëª¨ë“  ì´ì „ í† í°ì„ ë´¤ìŒ\n",
        "        # ë”°ë¼ì„œ ë¬¸ì¥ ì „ì²´ ì •ë³´ê°€ EoTì— ì••ì¶•ë˜ì–´ ìˆìŒ\n",
        "        #\n",
        "        # text.argmax(dim=-1): ê° ë¬¸ì¥ì—ì„œ ê°€ì¥ í° í† í° ID ì°¾ê¸°\n",
        "        # -> EoT í† í°ì˜ ìœ„ì¹˜ (EoTê°€ ê°€ì¥ í° special token ID)\n",
        "        #\n",
        "        # ì˜ˆ: text = [49406, 320, 1929, 49407, 0, 0, ...]\n",
        "        #     argmax = 3 (49407ì´ ê°€ì¥ í¼)\n",
        "        #     -> x[batch_idx, 3, :] ì¶”ì¶œ\n",
        "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
        "        # (batch, 512) @ (512, embed_dim) -> (batch, embed_dim)\n",
        "\n",
        "        return x\n",
        "\n",
        "    # ========================================\n",
        "    # ğŸ”¥ Forward Pass (ì „ì²´ ë™ì‘)\n",
        "    # ========================================\n",
        "    def forward(self, image, text):\n",
        "        \"\"\"\n",
        "        CLIPì˜ í•µì‹¬ forward pass\n",
        "\n",
        "        [ì…ë ¥]\n",
        "        - image: (batch, 3, 224, 224)\n",
        "        - text: (batch, 77)\n",
        "\n",
        "        [ì¶œë ¥]\n",
        "        - logits_per_image: (batch, batch) - ì´ë¯¸ì§€ ê¸°ì¤€ ìœ ì‚¬ë„ í–‰ë ¬\n",
        "        - logits_per_text: (batch, batch) - í…ìŠ¤íŠ¸ ê¸°ì¤€ ìœ ì‚¬ë„ í–‰ë ¬\n",
        "\n",
        "        [í•™ìŠµ ëª©í‘œ]\n",
        "        ëŒ€ê°ì„  ì›ì†Œê°€ ìµœëŒ€ê°€ ë˜ë„ë¡ (ë§¤ì¹­ë˜ëŠ” ìŒì˜ ìœ ì‚¬ë„ê°€ ë†’ê²Œ)\n",
        "        \"\"\"\n",
        "\n",
        "        # ===== Step 1: Encoding =====\n",
        "        # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ê°ê° ë²¡í„°ë¡œ ë³€í™˜\n",
        "        image_features = self.encode_image(image)    # (batch, embed_dim)\n",
        "        text_features = self.encode_text(text)       # (batch, embed_dim)\n",
        "\n",
        "        # ===== Step 2: Normalization â­ ë§¤ìš° ì¤‘ìš”! =====\n",
        "        # ë²¡í„°ë¥¼ ë‹¨ìœ„ ë²¡í„°ë¡œ ë§Œë“¦ (ê¸¸ì´ = 1)\n",
        "        #\n",
        "        # [ì™œ ì •ê·œí™”í•˜ëŠ”ê°€?]\n",
        "        # 1. ë²¡í„°ì˜ 'ê¸¸ì´'ê°€ ì•„ë‹Œ 'ë°©í–¥'ë§Œ ë¹„êµ\n",
        "        # 2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ê°€ëŠ¥\n",
        "        # 3. í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ\n",
        "        #\n",
        "        # ì •ê·œí™” ì•ˆ í•˜ë©´ í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§€ê³  ì„±ëŠ¥ ì €í•˜!\n",
        "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
        "\n",
        "        # ===== Step 3: Cosine Similarity ê³„ì‚° =====\n",
        "        #\n",
        "        # [Temperature Scaling]\n",
        "        # logit_scale = exp(Ï„) í˜•íƒœ\n",
        "        # ì´ˆê¸°ê°’: exp(log(1/0.07)) = 1/0.07 â‰ˆ 14.3\n",
        "        #\n",
        "        # [ì™œ exponential?]\n",
        "        # Ï„ë¥¼ ì§ì ‘ í•™ìŠµí•˜ë©´ ìŒìˆ˜ê°€ ë  ìˆ˜ ìˆìŒ\n",
        "        # expë¥¼ ì·¨í•˜ë©´ í•­ìƒ ì–‘ìˆ˜ ë³´ì¥\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "\n",
        "        # [Matrix Multiplication]\n",
        "        # image_features: (batch, embed_dim)\n",
        "        # text_features.t(): (embed_dim, batch)\n",
        "        # ê²°ê³¼: (batch, batch)\n",
        "        #\n",
        "        # [ì˜ë¯¸]\n",
        "        # logits[i, j] = ì´ë¯¸ì§€ iì™€ í…ìŠ¤íŠ¸ jì˜ ìœ ì‚¬ë„ * scale\n",
        "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
        "        logits_per_text = logits_per_image.t()  # Transpose\n",
        "\n",
        "        # ===== Step 4: Return =====\n",
        "        # í•™ìŠµ ì‹œ: ì´ logitsë¡œ cross entropy loss ê³„ì‚°\n",
        "        # ì¶”ë¡  ì‹œ: softmax ì·¨í•´ì„œ í™•ë¥ ë¡œ ë³€í™˜\n",
        "        return logits_per_image, logits_per_text"
      ]
    }
  ]
}